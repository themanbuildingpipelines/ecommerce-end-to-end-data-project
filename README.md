# E-Commerce Data Engineering Pipeline

An end-to-end data engineering project with a real-world multi-source e-commerce ecosystem with serious data quality issues. The pipeline implements medallion architecture (Bronze/Silver/Gold) to transform messy operational data into analytics-ready insights, uncovering **$120K+ in hidden revenue opportunities**. Read the full case study here. 

## Project Overview

This project demonstrates enterprise-grade data engineering practices by:
- Using 13 interconnected tables with data quality issues (12.6M rows, 2GB)
- Implementing medallion architecture for data transformation
- Building star schema models for analytics
- Identifying revenue leakage and operational inefficiencies
- Creating executive dashboards and actionable insights

**Business Impact**: Identified $120K in recoverable revenue across 7 opportunity categories including abandoned carts, failed payment retries, unprocessed refunds, and inventory optimization.

---

## Tools and Technologies

### Core Data Stack
- **Snowflake/BigQuery/PostgreSQL**: Cloud data warehouse (Bronze/Silver/Gold layers)
- **dbt**: SQL-based transformation framework for data modeling and quality
  - [dbt Documentation](https://docs.getdbt.com/)
- **Python**: Data generation and automation (Pandas, NumPy)
- **Tableau/Looker/Power BI**: Business intelligence and dashboards

### Supporting Technologies
- **Great Expectations**: Data quality validation and profiling
- **Airflow/Dagster** (optional): Orchestration for production pipelines
- **Git**: Version control and collaboration
- **SQL**: Primary transformation language

### Visualization & Documentation
- **Mermaid**: Architecture diagrams
- **Markdown**: Technical documentation

---

## Data Architecture

### Medallion Architecture Layers

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ BRONZE LAYER (Raw, Untouched)                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ 13 raw tables loaded from CSV files                       â”‚
â”‚ â€¢ No transformations applied                                â”‚
â”‚ â€¢ Metadata: load_timestamp, source_file, row_count         â”‚
â”‚ â€¢ Preserves all data quality issues for auditability       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SILVER LAYER (Cleaned, Standardized)                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ Data type corrections (string prices â†’ numeric)           â”‚
â”‚ â€¢ Deduplication (customers, transactions)                   â”‚
â”‚ â€¢ Standardization (categories, countries, statuses)         â”‚
â”‚ â€¢ Date/time normalization                                   â”‚
â”‚ â€¢ Referential integrity fixes                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ GOLD LAYER (Analytics-Ready, Star Schema)                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ Fact Tables: fact_orders, fact_payments, fact_sessions   â”‚
â”‚ â€¢ Dimension Tables: dim_customer, dim_product, dim_date     â”‚
â”‚ â€¢ Business metrics: revenue, CLV, return_rate, conversion   â”‚
â”‚ â€¢ Slowly changing dimensions (SCD Type 2)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Data Sources (13 Tables)

All data is **synthetically generated** to simulate real-world e-commerce operations:

### Source Systems (Operational)
1. **CRM Sales** (`crm_sales_01.csv`) - 1M rows
   - Customer intent and sales signals
   - Issues: 24-48h latency, 15% missing customer_id, duplicate sale_id
   
2. **E-Commerce Orders** (`ecom_orders_raw.csv`) - 2.7M rows
   - Transaction line items and order details
   - Issues: String prices, tax ambiguity, 3% duplicate transaction_id, 10% customer_id mismatch
   
3. **Product Inventory** (`inventory_master.csv`) - 50K rows
   - Product catalog and stock levels
   - Issues: Category chaos (47 variations), negative stock, reorder logic broken

### Master Data (Reference)
4. **Customer Master** (`customer_master_db.csv`) - 150K rows
   - Unified customer profile
   - Issues: 15% duplicates, email variations, address inconsistencies
   
5. **Sales Team Roster** (`sales_team_roster.csv`) - 50 rows
   - Employee and territory assignments
   - Issues: ID reuse (rehires), territory overlaps, missing effective dates
   
6. **Promotional Campaigns** (`promo_campaigns.csv`) - 100 rows
   - Marketing promotions and discount rules
   - Issues: Campaign ID reuse, date overlaps, free-text rules, usage exceeds limits

### Transactional Systems (Operations)
7. **Returns/Refunds** (`returns_log.csv`) - 300K rows
   - Product returns and refund processing
   - Issues: 20% missing records, dates before orders, refund > original amount
   
8. **Shipping/Fulfillment** (`shipment_tracking.csv`) - 1M rows
   - Logistics and delivery tracking
   - Issues: Multiple shipments per order, reused tracking numbers, date inversions
   
9. **Payment Transactions** (`payment_gateway_log.csv`) - 1.5M rows
   - Payment gateway events and retries
   - Issues: Multiple attempts per order, nested JSON, failed payments mixed with successful

### Customer Interaction (Support & Engagement)
10. **Support Tickets** (`support_tickets_raw.csv`) - 200K rows
    - Customer service interactions
    - Issues: 40% missing structured order_id (refs in free text), timezone chaos, low satisfaction coverage
    
11. **Email Marketing Events** (`email_campaign_events.csv`) - 750K rows
    - Email engagement funnel (sent/delivered/opened/clicked)
    - Issues: Broken conversion attribution (70% missing order_id), unsubscribes in separate system, A/B test variants unclear

### Marketing & Analytics (Attribution)
12. **Web Analytics Sessions** (`web_sessions_export.csv`) - 5M rows
    - User behavior and session tracking
    - Issues: 70% no customer_id (session_token only), negative time_on_site, UTM parameter mismatch
    
13. **Marketing Spend** (`ad_spend_daily.csv`) - 2K rows
    - Daily advertising costs by channel
    - Issues: Daily grain only, campaign name mismatch, clicks > impressions

---

## Project Structure

```
ecommerce-data-pipeline/
â”œâ”€â”€ data_generation/              # Python scripts for data generation
â”‚   â”œâ”€â”€ crm_sales_generator.py
â”‚   â”œâ”€â”€ ecommerce_orders.py
â”‚   â”œâ”€â”€ product_inventory.py
â”‚   â”œâ”€â”€ customer_master_generator.py
â”‚   â”œâ”€â”€ returns_refunds_generator.py
â”‚   â”œâ”€â”€ shipping_fulfillment_generator.py
â”‚   â”œâ”€â”€ payment_transactions_generator.py
â”‚   â”œâ”€â”€ sales_rep_generator.py
â”‚   â”œâ”€â”€ web_analytics_generator.py
â”‚   â”œâ”€â”€ marketing_spend_generator.py
â”‚   â”œâ”€â”€ promo_campaigns_generator.py
â”‚   â”œâ”€â”€ support_tickets_generator.py
â”‚   â”œâ”€â”€ email_marketing_generator.py
â”‚   â””â”€â”€ master_data_generator.py   # Runs all generators
â”‚
â”œâ”€â”€ dbt_project/                   # dbt transformation project
â”‚   â”œâ”€â”€ dbt_project.yml
â”‚   â”œâ”€â”€ profiles.yml
â”‚   â””â”€â”€ models/
â”‚       â”œâ”€â”€ bronze/                # Source definitions
â”‚       â”‚   â””â”€â”€ sources.yml
â”‚       â”œâ”€â”€ silver/                # Cleaned & standardized
â”‚       â”‚   â”œâ”€â”€ silver_orders_cleaned.sql
â”‚       â”‚   â”œâ”€â”€ silver_customers_deduped.sql
â”‚       â”‚   â”œâ”€â”€ silver_products_standardized.sql
â”‚       â”‚   â”œâ”€â”€ silver_returns_reconciled.sql
â”‚       â”‚   â””â”€â”€ silver_payments_success.sql
â”‚       â”œâ”€â”€ gold/                  # Star schema
â”‚       â”‚   â”œâ”€â”€ fact_orders.sql
â”‚       â”‚   â”œâ”€â”€ fact_payments.sql
â”‚       â”‚   â”œâ”€â”€ fact_sessions.sql
â”‚       â”‚   â”œâ”€â”€ dim_customer.sql
â”‚       â”‚   â”œâ”€â”€ dim_product.sql
â”‚       â”‚   â”œâ”€â”€ dim_date.sql
â”‚       â”‚   â””â”€â”€ dim_campaign.sql
â”‚       â””â”€â”€ analytics/             # Business insights
â”‚           â”œâ”€â”€ revenue_analysis.sql
â”‚           â”œâ”€â”€ customer_ltv.sql
â”‚           â”œâ”€â”€ hidden_revenue_opportunities.sql
â”‚           â””â”€â”€ operational_metrics.sql
â”‚
â”œâ”€â”€ tests/                         # dbt tests
â”‚   â”œâ”€â”€ data_quality/
â”‚   â””â”€â”€ business_logic/
â”‚
â”œâ”€â”€ docs/                          # Documentation
â”‚   â”œâ”€â”€ architecture_diagram.md
â”‚   â”œâ”€â”€ data_dictionary.md
â”‚   â””â”€â”€ data_quality_report.md
â”‚
â”œâ”€â”€ dashboards/                    # BI dashboard exports
â”‚   â”œâ”€â”€ revenue_leakage.twbx
â”‚   â”œâ”€â”€ operational_issues.twbx
â”‚   â”œâ”€â”€ customer_analytics.twbx
â”‚   â””â”€â”€ marketing_attribution.twbx
â”‚
â”œâ”€â”€ README.md                      # This file
â””â”€â”€ requirements.txt               # Python dependencies
```

---

## Data Flow

### 1. Generation Layer (Python)
```bash
# Generate all 13 tables with realistic data quality issues
python master_data_generator.py
```

**Output**: 13 CSV files (12.6M rows, ~2GB total)

### 2. Bronze Layer (Raw Ingestion)
```sql
-- Load CSVs into data warehouse as-is
-- No transformations, preserve all issues
CREATE TABLE bronze.crm_sales AS 
SELECT *, CURRENT_TIMESTAMP() AS load_timestamp 
FROM read_csv_auto('crm_sales_01.csv');
```

### 3. Silver Layer (dbt Transformations)
```bash
# Clean and standardize data
dbt run --models silver.*
```

**Key Transformations**:
- Parse string prices â†’ numeric
- Deduplicate customers (15% reduction)
- Standardize categories (47 variations â†’ 10 core)
- Fix date inversions
- Reconcile returns without refunds

### 4. Gold Layer (Star Schema)
```bash
# Build analytics-ready fact/dimension tables
dbt run --models gold.*
```

**Output**: 
- 3 fact tables (orders, payments, sessions)
- 4 dimension tables (customer, product, date, campaign)

### 5. Analytics Layer (Business Insights)
```bash
# Generate revenue opportunity analysis
dbt run --models analytics.*
```

**Output**: 
- $120K in hidden revenue opportunities
- 7 actionable insight queries
- Executive dashboards

---

## Key Data Quality Issues & Solutions

### Issue #1: 70% Web Sessions Lack Customer ID
**Problem**: Attribution impossible for majority of traffic

**Solution**:
```sql
-- Session stitching using cookies and IP address
WITH session_bridge AS (
    SELECT 
        session_token,
        customer_id,
        ip_address,
        ROW_NUMBER() OVER (PARTITION BY session_token ORDER BY session_start) AS rn
    FROM web_sessions
    WHERE customer_id IS NOT NULL
)
-- Probabilistic attribution model
```

### Issue #2: 15% Duplicate Customers
**Problem**: Inflated customer counts, loyalty program abuse

**Solution**:
```sql
-- Fuzzy matching on email, phone, address
WITH ranked_customers AS (
    SELECT *,
        ROW_NUMBER() OVER (
            PARTITION BY LOWER(TRIM(email))
            ORDER BY created_date DESC
        ) AS rn
    FROM bronze.customer_master
)
SELECT * FROM ranked_customers WHERE rn = 1
```

### Issue #3: String Prices ("$129.99", "â‚¬89.99")
**Problem**: Revenue calculations impossible

**Solution**:
```sql
-- Parse and convert to numeric
CAST(
    REGEXP_REPLACE(gross_price, '[^0-9.]', '') 
    AS DECIMAL(10,2)
) AS price_usd
```

### Issue #4: 20% Returns Not Logged
**Problem**: Revenue reconciliation broken, return rate underreported

**Solution**: Cross-reference payment refunds to identify missing returns

### Issue #5: Email Conversion Attribution Missing
**Problem**: 70-80% of click events lack order_id

**Solution**: Time-based probabilistic attribution (clicks within 24h of orders)

---

## $120K Hidden Revenue Analysis

### Revenue Opportunity Breakdown

| Opportunity | Amount | Effort | Priority | SQL Query |
|-------------|--------|--------|----------|-----------|
| **Abandoned Carts** | $25K | Low | ðŸ”¥ HIGH | `analytics/abandoned_carts.sql` |
| **Failed Payment Retries** | $28K | Low | ðŸ”¥ HIGH | `analytics/payment_retries.sql` |
| **Returns Without Refunds** | $22K | Medium | ðŸ”¥ HIGH | `analytics/unprocessed_refunds.sql` |
| **Promo Code Abuse** | $15K | Medium | ðŸŸ¡ MED | `analytics/promo_overuse.sql` |
| **Negative Inventory Sales** | $12K | High | ðŸŸ¡ MED | `analytics/inventory_lost_sales.sql` |
| **Shipping Cost Recovery** | $10K | Medium | ðŸŸ¢ LOW | `analytics/shipping_recovery.sql` |
| **Duplicate Account Credits** | $8K | Medium | ðŸŸ¢ LOW | `analytics/duplicate_credits.sql` |
| **TOTAL** | **$120K** | | | |

### Sample Insight Query

```sql
-- Abandoned Carts with Recovery Potential
SELECT
    customer_id,
    COUNT(*) as abandoned_carts,
    SUM(cart_value) as lost_revenue,
    AVG(DATEDIFF('day', cart_date, CURRENT_DATE())) as days_since_abandon
FROM {{ ref('silver_web_sessions') }}
WHERE order_status = 'abandoned'
  AND cart_value > 50
  AND DATEDIFF('day', cart_date, CURRENT_DATE()) BETWEEN 1 AND 30
GROUP BY customer_id
HAVING COUNT(*) >= 2
ORDER BY lost_revenue DESC
```

**Business Impact**: 5,000 customers with abandoned carts worth $25K. Single email campaign could recover 20% = $5K immediate revenue.

---

## Environment Setup

### Prerequisites
- Python 3.10+
- Snowflake/BigQuery/PostgreSQL account
- dbt-core or dbt Cloud
- Tableau/Looker/Power BI (for dashboards)
- Git

### Installation

1. **Clone repository**
```bash
git clone <repository-url>
cd ecommerce-data-pipeline
```

2. **Install Python dependencies**
```bash
pip install -r requirements.txt
```

3. **Configure dbt**
```bash
cd dbt_project
cp profiles.yml.example profiles.yml
# Edit profiles.yml with your database credentials
```

4. **Generate data**
```bash
cd ../data_generation
python master_data_generator.py
```

5. **Load to warehouse**
```bash
# Option 1: Snowflake (Python)
python load_to_snowflake.py

# Option 2: Manual upload via UI
# Upload CSVs to your warehouse
```

6. **Run dbt transformations**
```bash
cd ../dbt_project
dbt deps           # Install packages
dbt seed           # Load seed data (if any)
dbt run            # Run all models
dbt test           # Run data quality tests
```

---

## Quick Start

### Option 1: Full Pipeline (Recommended)
```bash
# 1. Generate all data (~1-2 hours)
python data_generation/master_data_generator.py

# 2. Load to warehouse (varies by method)
python load_to_snowflake.py

# 3. Run dbt transformations (~10-15 min)
cd dbt_project
dbt run

# 4. Generate dashboards (manual in BI tool)
```

### Option 2: Sample Data (Fast Testing)
```bash
# Generate 10K row samples (~5 minutes)
python data_generation/master_data_generator.py --sample 10000

# Load and transform
python load_to_snowflake.py --sample
cd dbt_project
dbt run --select silver.* gold.*
```

---

## Testing

### dbt Tests
```bash
cd dbt_project

# Run all tests
dbt test

# Test specific models
dbt test --select silver_orders_cleaned
dbt test --select gold.*
```

### Data Quality Tests
- **Uniqueness**: Primary keys have no duplicates
- **Not Null**: Required fields are populated
- **Referential Integrity**: Foreign keys match parent tables
- **Accepted Values**: Status fields match allowed values
- **Relationships**: Orders link to valid customers/products

### Python Tests (Optional)
```bash
cd data_generation
pytest tests/ -v
```

---

## Deployment

### Local Development
```bash
# Run dbt locally
dbt run --target local
```

### Production
```bash
# Deploy to production
dbt run --target prod

# Run incremental models only
dbt run --select state:modified+ --state ./target
```

### CI/CD (GitHub Actions Example)
```yaml
name: dbt CI/CD
on: [push]
jobs:
  dbt-run:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - name: Install dbt
        run: pip install dbt-snowflake
      - name: Run dbt
        run: |
          cd dbt_project
          dbt run --target prod
          dbt test
```

---

## Dashboards

### Executive Dashboard Suite (4 Dashboards)

1. **Revenue Leakage Dashboard**
   - KPI Cards: $120K total opportunity, $75K high-priority
   - Abandoned cart trend (daily)
   - Failed payments by reason
   - Unprocessed refunds aging report

2. **Operational Issues Dashboard**
   - Negative inventory products (top 15)
   - Duplicate customer accounts
   - Promo code overuse alerts
   - Shipping cost errors

3. **Customer Analytics Dashboard**
   - Customer lifetime value distribution
   - Cohort retention curves
   - Churn risk segmentation
   - Email engagement funnel

4. **Marketing Attribution Dashboard**
   - Channel ROI comparison
   - Campaign performance table
   - Web-to-order conversion rates
   - Email click-to-purchase tracking

---

## Project Metrics

| Metric | Value |
|--------|-------|
| **Total Tables** | 13 |
| **Total Rows** | 12.6M |
| **Total Storage** | ~2GB |
| **Data Quality Issues** | 45+ types |
| **dbt Models** | 25+ |
| **Revenue Opportunities** | $120K |
| **Generation Time** | 1.5-2 hours |
| **Transformation Time** | 10-15 min |

---

## Skills Demonstrated

### Data Engineering
- âœ… Medallion architecture (Bronze/Silver/Gold)
- âœ… Star schema design (fact/dimension modeling)
- âœ… dbt transformations and testing
- âœ… Data quality management
- âœ… ETL/ELT pipelines
- âœ… Slowly changing dimensions (SCD Type 2)

### Analytics Engineering
- âœ… Business metrics definition
- âœ… Customer lifetime value (CLV)
- âœ… Cohort analysis
- âœ… Attribution modeling
- âœ… Revenue reconciliation

### Technical Skills
- âœ… Python (Pandas, NumPy)
- âœ… SQL (complex joins, window functions, CTEs)
- âœ… Cloud data warehouses (Snowflake/BigQuery)
- âœ… Data visualization (Tableau/Looker)
- âœ… Git version control
- âœ… Documentation (Markdown, Mermaid)

---

## Use Cases

### For Data Engineers
- Portfolio project demonstrating real-world data pipeline
- Practice with messy data and quality issues
- Learn dbt best practices
- Implement medallion architecture

### For Analytics Engineers
- Build star schema models
- Create business metrics
- Practice SQL optimization
- Design executive dashboards

### For Hiring Managers
- Assess candidate's ability to:
  - Handle complex data quality issues
  - Build scalable data pipelines
  - Generate business insights
  - Communicate technical concepts

---

## Future Enhancements

- [ ] Airflow/Dagster orchestration
- [ ] Great Expectations data quality framework
- [ ] CI/CD pipeline with automated tests
- [ ] Machine learning models (churn prediction, LTV forecasting)
- [ ] Real-time streaming layer (Kafka/Kinesis)
- [ ] Data catalog (DataHub/Atlan)
- [ ] Cost optimization analysis

---

## Contributing

Contributions welcome! Please:
1. Fork the repository
2. Create a feature branch
3. Add tests for new functionality
4. Submit a pull request

---

## License

MIT License - feel free to use for personal or commercial projects.

---

## Contact & Acknowledgments

**Author**: [Your Name]  
**LinkedIn**: [Your LinkedIn]  
**Portfolio**: [Your Portfolio Site]  

**Inspired by**: Real-world e-commerce data engineering challenges at mid-sized DTC brands.

---

## Additional Resources

- [dbt Best Practices](https://docs.getdbt.com/best-practices)
- [Kimball Dimensional Modeling](https://www.kimballgroup.com/data-warehouse-business-intelligence-resources/kimball-techniques/dimensional-modeling-techniques/)
- [Snowflake Documentation](https://docs.snowflake.com/)
- [Tableau Training](https://www.tableau.com/learn/training)

---

*This project demonstrates production-grade data engineering practices and business acumen suitable for senior data engineering roles.*
